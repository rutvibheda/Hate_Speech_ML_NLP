{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentistrength import PySentiStr\n",
    "import pandas as pd\n",
    "\n",
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('C:/Users/femin/MP_Final/SentiStrength.jar') # Note: Provide absolute path instead of relative path\n",
    "senti.setSentiStrengthLanguageFolderPath('C:/Users/femin/MP_Final/SentiStrength/') # Note: Provide absolute path instead of relative path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import ast\n",
    "ps = PorterStemmer()\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\femin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'RT', '', tweet)\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = splitTags(tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "#             tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTags(s):\n",
    "    l = [i  for i in s.split() if i.startswith(\"#\") ]\n",
    "    for i in l:\n",
    "        expanded = \" \".join([a for a in re.split('([A-Z][a-z]+)', i[1:]) if a])\n",
    "        s = s.replace(i,expanded)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stemming(processed):\n",
    "    stemmed = [ps.stem(i) for i in processed if i != '']\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiCalc(tweet):\n",
    "    scale = senti.getSentiment(tweet, score = 'scale')\n",
    "    dual = senti.getSentiment(tweet, score = 'dual')\n",
    "    binary = senti.getSentiment(tweet, score = 'binary')\n",
    "    return [scale[0],dual[0][0],dual[0][1],binary[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_x(tweet):\n",
    "    processed = process_tweet(tweet)\n",
    "    print(\"\\nPreProcessing and Tokenization\")\n",
    "    print(processed)\n",
    "    stemmed = stemming(processed)\n",
    "    print(\"\\nStemming\")\n",
    "    print(stemmed)\n",
    "    sentiment = sentiCalc(tweet)\n",
    "    print(\"\\nSentiment(Scale,Pos,Neg,Binary)\")\n",
    "    print(sentiment)\n",
    "    semantics = generate_punctuation_rows(tweet)\n",
    "    print(\"\\nSemantics (QuestionMarkCount, QuotationMarkCount, ExclamationMarkCount, AllCapitalised) \")\n",
    "    print(semantics)\n",
    "    unis = generate_col(stemmed,cols)\n",
    "    print(\"\\nUnigrams\")\n",
    "    print(unis)\n",
    "#     print(processed,stemmed,sentiment,unis,semantics)\n",
    "    return processed,stemmed,sentiment,unis,semantics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "with open('C:/Users/femin/MP_Final/combined.txt') as f:\n",
    "    cols = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_col(row,col):\n",
    "    uni = []\n",
    "    for i in col:\n",
    "        if i in row:\n",
    "            uni.append(1)\n",
    "        else:\n",
    "            uni.append(0)\n",
    "    return uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_punctuation_rows(tweet):\n",
    "#     QuestionMarkCount | QuotationMarkCount | ExclamationMarkCount | AllCapitalised\n",
    "    x, p= process_tweet2(tweet) # need to use process tweet to remove redundant exclamation and RT string\n",
    "#     print(p)\n",
    "    res=[]\n",
    "    if '?' in p: \n",
    "        res.append(p['?']) \n",
    "    else:\n",
    "        res.append(0)\n",
    "    if '\\'' in p: \n",
    "        res.append(p['\\'']) \n",
    "    else:\n",
    "        res.append(0)\n",
    "    if '!' in p: \n",
    "        res.append(p['!']) \n",
    "    else: \n",
    "        res.append(0)\n",
    "   \n",
    "    res.append(p['allCaps']) \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation_and_allcaps(tweet):\n",
    "    p={'allCaps':0}\n",
    "    if type(tweet)==str:\n",
    "        tweet_str= tweet\n",
    "    else:\n",
    "        tweet_str=' '.join(tweet)\n",
    "#     print(tweet_str)\n",
    "    temp= re.findall('[A-Z]{2,}', tweet_str)\n",
    "#     print(temp)\n",
    "    p['allCaps']=len(temp)\n",
    "    \n",
    "    for i in tweet:\n",
    "        if i in string.punctuation and i not in ['@','#'] and i not in p:\n",
    "            p[i]=tweet.count(i)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet2(tweet):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    tweet= re.sub(\"!*\\sRT?\", ' ', tweet)\n",
    "    tweet= re.sub(\"\\A!*\", '',  tweet)\n",
    "    tweet= re.sub(\"\\A\\sRT\\s\", ' ',  tweet)\n",
    "    \n",
    "    x= count_punctuation_and_allcaps(tweet)\n",
    "    tweet = re.sub(r'RT', '', tweet)\n",
    "    tweet = re.sub(r'@[A-Za-z0-9]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = splitTags(tweet)\n",
    "    \n",
    "# %%%%%%% changed preserve case parameter to True\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "#     print(tweet,tweet_tokens)\n",
    "    \n",
    "#     print(x)\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in string.punctuation):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "#             tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euphemism(name):\n",
    "    synonyms = []\n",
    "    for syn in wn.synsets(name):\n",
    "        for hyp in syn.hypernyms():\n",
    "            if wn.wup_similarity(hyp, syn)>0.1:\n",
    "                for l in hyp.lemmas():\n",
    "                    synonyms.append(l.name().replace('_',' ')) \n",
    "    return synonyms[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_x = sentiment[:0] + semantics + unis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_x = np.reshape(final_x,(1,-1))\n",
    "# print(final_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkl_Filename = \"ternary_model.pkl\"  \n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    lr = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred=lr.predict(final_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output(tweet):\n",
    "    processed,stemmed,sentiment,unis,semantics = generate_x(tweet)\n",
    "    euphems = generate_euphemisms(processed,stemmed)\n",
    "    final_x = sentiment + semantics + unis\n",
    "    final_x = np.reshape(final_x,(1,-1))\n",
    "    print(\"\\nFinal Input Vector: \\n\",final_x)\n",
    "    Pkl_Filename = \"ternary_model.pkl\"  \n",
    "    with open(Pkl_Filename, 'rb') as file:  \n",
    "        lr = pickle.load(file)\n",
    "    prob = (lr.predict_proba(final_x))[0]\n",
    "    \n",
    "    print('\\n\\nClass Probablities')\n",
    "    \n",
    "    [print(\"{0:.5f}\".format(i)) for i in prob]\n",
    "    y_pred=lr.predict(final_x)\n",
    "\n",
    "    if y_pred[0] == 0:\n",
    "        return 'Hateful',euphems\n",
    "    else :\n",
    "        return 'Clean',euphems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PreProcessing and Tokenization\n",
      "['answer', 'snapchat', 'faggot', 'butt', 'hurt']\n",
      "\n",
      "Stemming\n",
      "['answer', 'snapchat', 'faggot', 'butt', 'hurt']\n",
      "\n",
      "Sentiment(Scale,Pos,Neg,Binary)\n",
      "[-1, 1, -2, -1]\n",
      "\n",
      "Semantics (QuestionMarkCount, QuotationMarkCount, ExclamationMarkCount, AllCapitalised) \n",
      "[0, 0, 0, 0]\n",
      "\n",
      "Unigrams\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Final Input Vector: \n",
      " [[-1  1 -2 -1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0]]\n",
      "\n",
      "\n",
      "Class Probablities\n",
      "0.99672\n",
      "0.00328\n",
      "\n",
      "\n",
      "faggot :  ['gay man', 'shirtlifter', 'bundle', 'sheaf', 'embroider', 'broider', 'tie down', 'tie up', 'bind', 'truss', 'tie down', 'tie up', 'bind', 'truss']\n",
      "\n",
      "Prediction:  Hateful\n"
     ]
    }
   ],
   "source": [
    "s = ' @Shun_stokes: This is a really good book to read. Are you going to read it?'\n",
    "classes ,euphem = final_output(s)\n",
    "print('\\n')\n",
    "for k in euphem:\n",
    "    print(k,\": \",euphem[k])\n",
    "    print()\n",
    "# print(euphem)\n",
    "print('Prediction: ',classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_euphemisms(processed,stemmed):\n",
    "    cols = []\n",
    "    with open('C:/Users/femin/MP_Final/combined.txt') as f:\n",
    "        cols = f.read().splitlines()\n",
    "    d = {}\n",
    "    for i in range(len(processed)):\n",
    "        if stemmed[i] in cols:\n",
    "            s = euphemism(processed[i])\n",
    "            d[processed[i]]=euphemism(processed[i])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euphemism('bitch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- &#8220;@NoChillPaz: White kids favorite activities 1. Calling people \"niggers\" on Xbox 2.fucking their pet 3. Shooting up their school&#8221;\n",
    "\n",
    "- @AustinG1135 answer my snapchat faggot #ButtHurt\n",
    "\n",
    "- @IGGYAZALEA it was a joke you fugly bitch be happy uncle snoop even acknowledged your nasty ass lmao\n",
    "\n",
    "- @Shun_stokes: My cousin said he share bitches with his brothers. He said sharing is caring and he love his brothers and the sluts &#128514'\n",
    "\n",
    "- @MiAmor Yo he just a retarded spic, tell that nigga to go fuck himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
